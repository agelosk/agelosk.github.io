<!DOCTYPE HTML>
<html lang="en">
  <head>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FNR5STJD9R"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-FNR5STJD9R');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Agelos Kratimenos</title>

    <meta name="author" content="Agelos Kratimenos">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Agelos Kratimenos
                </p>
                  <p align="justify"> I'm a PhD student at the GRASP Lab of the University of Pennsylvania, advised by <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>. 
                    Previously, I completed my BSc & MEng in Electrical and Computer Engineering at the National Technical University of Athens (NTUA), where I was honored to work with
                     <a href="https://robotics.ntua.gr/members/maragos/">Petros Maragos</a>.
                </p>
                <p>
                </p>
                <p style="text-align:center">
                  <a href="mailto:agelosk@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=WbgY21EAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/agelosk//">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/agelos.jpg"><img style="width:100%;max-width:100%;object-fit: cover "" alt="profile photo" src="images/agelos.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests are in the area of 3D computer vision and specifically in Dynamic neural rendering, motion decomposition, tracking, and human reconstruction.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='dynmf_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/dynmf.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/dynmf.jpg' width="160">
    </div>
    <script type="text/javascript">
      function difsurvey_start() {
        document.getElementById('dynmf_image').style.opacity = "1";
      }

      function difsurvey_stop() {
        document.getElementById('dynmf_image').style.opacity = "0";
      }
      difsurvey_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2312.00112">
      <span class="papertitle">DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting
</span>
    </a>
    <br>
  <strong>Agelos Kratimenos</strong>,
  <a href="https://www.cis.upenn.edu/~leijh/">Jiahui Lei</a>,
	<a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,
    <br>
	<em>ECCV<em>, 2024
    <br>
        <a href="https://agelosk.github.io/dynmf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2312.00112">arXiv</a>
        /
        <a href="https://github.com/agelosk/dynmf">Github</a>
        <p></p>
    <p></p>
    <p>
      DynMF is a sparse trajectory decomposition that enables robust per-point tracking. 
      In addition to NVS, it allows us to control trajectories, enable/disable them, leading to new ways of video editing, 
      dynamic motion decoupling, and novel motion synthesis. 
    </p>
  </td>
</tr>          

  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='slr_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/slr.jpg" >
    Your browser does not support the video tag.
    </video></div>
      <img src='images/slr.jpg' width="180">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2012.05698">
      <span class="papertitle">Independent Sign Language Recognition with 3D Body, Hands, and Face Reconstruction
</span>
    </a>
    <br>
  <strong>Agelos Kratimenos</strong>,
  <a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>,
	<a href="https://robotics.ntua.gr/members/maragos/">Petros Maragos</a>,
    <br>
	<em>ICASSP<em>, 2021
    <br>
        
        <a href="https://arxiv.org/abs/2012.05698">arXiv</a>
        <p></p>
    <p></p>
    <p>
      We employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body
    shape, face and hands information from a single image. We use this holistic 3D reconstruction for the Sign Language Recognition task.
    </p>
  </td>
</tr>       

  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='slr_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/icassp2.png" >
    Your browser does not support the video tag.
    </video></div>
      <img src='images/icassp2.png' width="180">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/document/9413479">
      <span class="papertitle">Deep Convolutional and Recurrent Networks for Polyphonic Instrument Classification from Monophonic Raw Audio Waveforms
</span>
    </a>
    <br>
    <a href="https://klean2050.github.io/">Kleanthis Avramidis*</a>,
    <strong>Agelos Kratimenos*</strong>,
    <a href="http://cvsp.cs.ntua.gr/garoufis/index.shtm">Christos Garoufis</a>,
    <a href="https://scholar.google.gr/citations?user=5ll6AGgAAAAJ&hl=en">Athanasia Zlatintsi</a>,
	  <a href="https://robotics.ntua.gr/members/maragos/">Petros Maragos</a>,
    <br>
	<em>ICASSP<em>, 2021
    <br>
        
        <a href="https://ieeexplore.ieee.org/document/9413479">arXiv</a>
        <p></p>
    <p></p>
    <p>
      we attempt to recognize musical instruments in polyphonic audio by only feeding their raw waveforms 
      into deep learning models. Various recurrent and convolutional architectures incorporating residual 
      connections are examined and parameterized in order to build end-toend classifiers with low computational cost and only minimal preprocessing.
    </p>
  </td>
</tr>     

  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='slr_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/eusipco.png" >
    Your browser does not support the video tag.
    </video></div>
      <img src='images/eusipco.png' width="180">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/1911.12505">
      <span class="papertitle">Augmentation Methods on Monophonic Audio for Instrument Classification in Polyphonic Music
</span>
    </a>
    <br>
    <strong>Agelos Kratimenos*</strong>,
    <a href="https://klean2050.github.io/">Kleanthis Avramidis*</a>,
    <a href="http://cvsp.cs.ntua.gr/garoufis/index.shtm">Christos Garoufis</a>,
    <a href="https://scholar.google.gr/citations?user=5ll6AGgAAAAJ&hl=en">Athanasia Zlatintsi</a>,
	  <a href="https://robotics.ntua.gr/members/maragos/">Petros Maragos</a>,
    <br>
	<em>EUSIPCO<em>, 2020
    <br>
        
        <a href="https://arxiv.org/abs/1911.12505">arXiv</a>
        /
        <a href="https://github.com/agelosk/Instrument_Classification_Paper">Github</a>
        <p></p>
    <p></p>
    <p>
      We present an approach for instrument classification in polyphonic music using monophonic training data that involves mixing-augmentation methods. 
      Specifically, we experiment with pitch and tempo-based synchronization, as well as mixes of tracks with similar music genres.
    </p>
  </td>
</tr>      
    
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Another Jon Barron's website.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
