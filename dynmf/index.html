
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title> DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting </title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="img/seal_icon.png">
    <meta property="og:image:type" content="img/seal_icon.png">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://agelosk.github.io/dynmf/"/>
    <meta property="og:title" content="DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting" />
    <meta property="og:description" content="Project page for DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting" />
    <meta name="twitter:description" content="Project page for DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting." />
    <meta name="twitter:image" content="img/seal_icon.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="img/seal_icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110862391-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>DynMF</b>: Neural Motion Factorization <br> 
                for Real-time Dynamic View Synthesis with 3D Gaussian Splatting  </br> 
                <small>
                    <!--NeurIPS 2020 (spotlight)-->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://agelosk.github.io/">
                          Agelos Kratimenos
                        </a>
                        <br></br>
                    </li>
                    <li>
                        <a href="https://www.cis.upenn.edu/~leijh/">
                          Jiahui Lei
                        </a>
                        </br>University of Pennsylvania
                    </li>
                    <li>
                        <a href="https://www.cis.upenn.edu/~kostas/">
                            Kostas Daniilidis
                        </a>
                        <br></br>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/">
                            <image src="img/ff_paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/agelosk/dynmf">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                                <h9> Coming soon! </h9>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                Accurately and efficiently modeling dynamic scenes and motions is considered so challenging a task due to temporal
                dynamics and motion complexity. To address these challenges, we propose DynMF, a compact and efficient representation that 
                decomposes a dynamic scene into a few neural trajectories. We argue that the per-point motions of a dynamic scene can be decomposed into a small set
                of explicit or learned trajectories. Our carefully designed neural framework consisting of a tiny set of learned basis
                queried only in time allows for rendering speed similar to 3D Gaussian Splatting, surpassing 120 FPS, while at the
                same time, requiring only double the storage compared to static scenes. Our neural representation adequately constrains the inherently 
                underconstrained motion field of a dynamic scene leading to effective and fast optimization.
                This is done by biding each point to motion coefficients that enforce the per-point sharing of basis trajectories. By
                carefully applying a sparsity loss to the motion coefficients, we are able to disentangle the motions that comprise the
                scene, independently control them, and generate novel motion combinations that have never been seen before. We can
                reach state-of-the-art render quality within just 5 minutes of training and in less than half an hour, we can synthesize
                novel views of dynamic scenes with superior photorealistic quality. Our representation is interpretable, efficient, and
                expressive enough to offer real-time view synthesis of complex dynamic scene motions, in monocular and multi-view scenarios.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="img/presentation.mp4#t=0.8" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <p style="text-align:center;">
                    <image src="img/main.jpg" width="750px" class="center">
                </p>
                <p class="text-justify">
                    <b>Overview of DynMF:</b> The underlying dense motion field (left-top) of a dynamic scene is factorized into a set of globally shared
                    learnable motion basis (left-bottom) and their motion coefficients stored on each Gaussian (right-bottom). Given a query time t, the
                    deformation can be efficiently computed via a single global forward of the motion basis and the motion coefficient blending (middle-
                    bottom) to recover the deformed scene (top-right).  </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Trajectory tracking
                </h3>
                <video id="v0" width="49%" autoplay loop muted controls>
                  <source src="img/traj/bb_traj.mp4"  type="video/mp4"/> 
                </video>
                <video id="v0" width="49%" autoplay loop muted controls>
                    <source src="img/traj/jj_traj.mp4"  type="video/mp4"/>
                </video>                
                <video id="v0" width="49%" autoplay loop muted controls>
                    <source src="img/traj/mutant_traj.mp4"  type="video/mp4"/>
                </video>
                <video id="v0" width="49%" autoplay loop muted controls>
                    <source src="img/traj/standup_traj.mp4"  type="video/mp4"/>
                </video>
                <p class="text-justify">
                    We manage to expressively model scene element deformation of complex dynamics, through a simple and interpretable framework. 
                    Our method enables robust per-point tracking, overcoming displacement ambiguities, overlappings, and non-rigid complex motions. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motion Decomposition
                </h3>
                <video id="v0" width="24.5%" autoplay loop muted controls onloadstart="this.playbackRate = 1.127;"
                  src="img/decomp/bb_decomp_blueball.mp4">
                </video>
                <video id="v0" width="24.5%" autoplay loop muted controls>
                    <source src="img/decomp/bb_decomp_greenball.mp4"  type="video/mp4"/>
                </video>                
                <video id="v0" width="24.5%" autoplay loop muted controls onloadstart="this.playbackRate = 0.82925;"
                    src="img/decomp/mutant_decomp_lefthand.mp4">
                </video>
                <video id="v0" width="24.5%" autoplay loop muted controls onloadstart="this.playbackRate = 0.58125;"
                    src="img/decomp/mutant_decomp_righthand.mp4">
                </video>
                <p class="text-justify">
                    A key component of our proposed representation is its ability to explicitly decompose each dynamic scene 
                    to its core independent motions. Specifically, by applying a sparsity loss, 
                    we enforce each Gaussian to choose only one of the few trajectories available. This design combined with the 
                    inherent rigid property of our representation drives all nearby Gaussians to choose the same and only trajectory. 
                    This strongly increases the controllability of the dynamic scene, by disentangling motions, allowing for novel 
                    scene creation, interactively choosing which part of the scene is moving, and so on. Notice how only the blue or the green ball 
                    is moving in the 'Bouncingballs' scene or only the left or the right hand is moving in the 'Mutant' D-NeRF scene. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motion editing
                </h3>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/video_edt.mp4"  type="video/mp4"/> 
                </video>
                <p class="text-justify">
                    Being able to efficiently factorize all the motions of a dynamic scene into a few basis trajectories, allows us 
                    to control these trajectories, enable or disable them, leading to new ways of video editing.
                    Here, we demo that in the original rendering of the 'Flame steak' DyNeRF scene the window blinds are moving.
                    With our motion decomposition framework, we can isolate this movement, disable it, and have the original 
                    dynamic rendered scene without such potentially unwanted background movement. 
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Visualization of basis trajectories
                </h3>
                <video id="v0" width="49.8%" autoplay loop muted controls>
                  <source src="img/basis/video.mp4"  type="video/mp4"/> 
                </video>
                <video id="v0" width="49.8%" autoplay loop muted controls>
                    <source src="img/basis/video_decomp.mp4"  type="video/mp4"/>
                </video>                

                <p class="text-justify">
                This figure depicts the learned trajectories that can model the dynamics of a scene. Specifically, each Gaussian can choose one or 
                more of these 10 trajectories to model its unique motion in the dynamic scene. When we allow a linear combination of the 10 trajectories, 
                then the basis functions are uniformly spread in the 3D world (left video). This is because each Gaussian 
                can model its unique motion by linearly combining these motions, which lets it uniformly move in the space of the dynamic scene. If we 
                restrict each Gaussian to choose only one trajectory, then these become more odd and specific to the motion needs of the corresponding scene (right video).
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dynamic Rendering Results
                </h3>
                <video id="v0" width="49%" autoplay loop muted controls>
                  <source src="img/rendering/jj_multiview.mp4"  type="video/mp4"/> 
                </video>
                <video id="v0" width="49%" autoplay loop muted controls>
                    <source src="img/rendering/bb_multiview.mp4"  type="video/mp4"/>
                </video>                
                <video id="v0" width="49%" autoplay loop muted controls>
                    <source src="img/rendering/mutant_multiview.mp4"  type="video/mp4"/>
                </video>
                <video id="v0" width="49%" autoplay loop muted controls>
                    <source src="img/rendering/trex_multiview.mp4"  type="video/mp4"/>
                </video>
                <video id="v0" width="98.42%" autoplay loop muted controls>
                    <source src="img/rendering/flamesteak_static.mp4"  type="video/mp4"/>
                </video>
                <p class="text-justify">
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-14 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" readonly>
@article{kratimenos2024dynmf,
    title={DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting},
    author={Agelos Kratimenos and Jiahui Lei and Kostas Daniilidis},
    journal={arXiV},
    year={2023}
}</textarea>
                </div>
            </div>
        </div>

    </div>
</body>
</html>
